<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>SWE-smith</title>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/home.css" />
  <link rel="stylesheet" href="../css/carousel.css" />
  <link rel="icon" href="../assets/home/swesmith.png" type="image/x-icon" />
</head>
<body>
  <nav>
    <ul class="nav-bar">
      <li><a href="https://huggingface.co/SWE-bench">Data</a></li>
      <li><a href="https://arxiv.org/abs/2504.21798">Paper</a></li>
      <li><a href="https://github.com/SWE-bench/SWE-smith">Code</a></li>
      <li><a href="/getting_started">Docs</a></li>
    </ul>
  </nav>

  <main>
    <h1 class="title fire-text">
      <img
        src="../assets/home/swesmith.png"
        style="width:2em;height:2em;vertical-align: middle;font-size:2.5rem;"
        alt="SWE-smith logo" />SWE-smith
    </h1>
    <h3 class="subtitle">
      Scaling Data for Software Engineering Agents
    </h3>

    <section class="blog">
      <p style="color:#777;letter-spacing: -0.05rem;">
        <a href="index.html" style="color:#777">Home</a> &bull; April 30, 2025
      </p>
      <p>by <a href="https://john-b-yang.github.io/">John Yang</a></p>
      <p style="margin-top:1.5rem;">
        A rethinking of how to create software tasks for training AI.
      </p>
      <h3 class="fire-text">Motivations</h3>
      <p>
        Progress in AI for software engineering over the past year has been
        incredibly exciting.
        The rapid development of agentic SWEs has been tracked faithfully by the
        <a href="https://swe-bench.github.io/">SWE-bench</a> benchmark and the
        <a href="https://openai.com/index/introducing-swe-bench-verified/">Verified</a>
        subset in particular.
      </p>
      <p>
        For well over a year, I've been the core maintainer of SWE-bench's
        <a href="https://github.com/swe-bench/experiments">leaderboard</a>,
        giving me a front row seat to the show.
      </p>
      <img
        src="../assets/home/leaderboard.png"
        alt="SWE-bench leaderboard"
        style="width:100%;display:block;margin:1.5rem auto;border-radius:1em;" />
      <p>
        Early improvements were driven by creating better inference frameworks for
        interacting with code, such as
        <a href="https://swe-agent.com/latest/">SWE-agent</a>,
        <a href="https://www.all-hands.dev/">OpenHands</a>,
        and <a href="https://github.com/OpenAutoCoder/Agentless">Agentless</a>.
      </p>
      <p>
        Since late last summer, that trend has shifted towards better base models.
        For the SWE-bench team and myself,
        <a href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude 3.5 Sonnet</a>
        was a big moment, eclipsing 50% on SWE-bench Verified.
      </p>
      <p>
        The attention of the community followed.
        For open source to play catch-up, we need data and execution environments.
      </p>
      <p>
        More than before, GitHub issues on SWE-bench popped up requesting more
        documentation on how to re-do SWE-bench for new repositories.
        (see
        <a href="https://github.com/SWE-bench/SWE-bench/issues/334">1</a>,
        <a href="https://github.com/SWE-bench/SWE-bench/issues/347">2</a>,
        <a href="https://github.com/SWE-bench/SWE-bench/issues/364">3</a>,
        <a href="https://github.com/SWE-bench/SWE-bench/issues/333">4</a>,
        <a href="https://github.com/SWE-bench/SWE-bench/issues/287">5</a>
        + many, many more)
      </p>
      <p>
        In early 2025, we've seen signals of success for training open source SWE-agents.
        Prior and concurrent works such as
        <a href="https://github.com/SWE-Gym/SWE-Gym">SWE-gym</a>,
        <a href="https://github.com/facebookresearch/swe-rl/">SWE-RL</a>,
        and <a href="https://r2e-gym.github.io/">R2E-gym</a>
        have shown how, with enough data, simple SFT or GRPO objectives
        lead to promising performance gains.
        SWE-gym trains on 491 agent trajectories, R2E-gym on 3200.
        SWE-RL, which is non-agentic, trains on 11M PRs synthesized into reasoning traces.
        Their single run % resolve rates are 20.6%, 34.4%, and 41% respectively.
      </p>
      <p>
        However, the foundation to all this progress - collecting training
        data for AI software engineers -
        remains a significant pain point.
      </p>
      <h3 class="fire-text">What's so hard about SWE-bench?</h3>
      <p>
        As a reminder, here's how we built SWE-bench.
      </p>
      <img
        src="../assets/home/collection.png"
        alt="SWE-bench overview"
        style="width:100%;display:block;margin:1.5rem auto;"
      />
      <p>
        You scrape a bunch of PRs and convert them to candidate task instances.
        Then, you create an execution environment per candidate and run the tests,
        checking for at least 1 "Fail to Pass" test
        (the test failed before the PR, and passed after).
      </p>
      <p>
        This last step, creating the execution environment, is tough.
      </p>
      <ol>
        <li><span style="color:#ff2d00">Human Labor</span>:
          Each task instance is from a unique commit for that repository.
          Figuring out how to install the repository and run the tests 
          requires a lot of manual labor.
          <br><br>
          Imagine, you find the installion docs or contributing guidelines.
          After reading it, you try it out.
          Sometimes it works, most of the time it doesn't.
          Why? Because the commit is from 2021, and the latest version of
          some dependencies are not compatible.
          So you then look up the package on PyPI and install the version
          closest to the commit date.
          Great, that worked!
          But uh-oh, "libGl.so.1: cannot open shared object file", what does
          that mean?
          Ok, turns out you need to apt-get some packages.
          I'm done! Wait.
          What?? Why does this version of the package not exist anymore?
          Repeat 5 to âˆž times, and at best, you have one
          execution environment.
          For one task instance.
          And 45 minutes have passed.
          <br><br>
          ðŸ˜«
          <br><br>
          Again, that's at best.
          I cannot tell you how many repo's I've given up on figuring out
          when putting together SWE-bench and
          <a href="https://www.swebench.com/multimodal.html">SWE-bench Multimodal</a>.
          The iteration cycles were worse for non-Python.
          <br><br>
          There are of course, ways you can be clever about this.
          Notably, in SWE-bench, per repository, we group the task instances with
          the same version number and define repository/version specific specs.
          <br><br>
          But the problem remains at large.
          When we built SWE-bench, scraping PRs took 1 week end-to-end (code + collection).
          Execution environments took 10 weeks of trial and error.
          And then another 8 weeks
          <a href="https://github.com/swe-bench/SWE-bench/blob/main/docs/20240627_docker/README.md">
            overhauling the existing codebase</a>  for SWE-bench Verified.
          To this day, we're still debugging environment issues.
          Even with SWE-bench as precedent, SWE-gym reports taking 200 human hours.
          <br><br>
          And as humans, we only have so much patience.
          No SWE-bench style training set captures more than 11 repositories.
        </li>
        <li><span style="color:#ff2d00;">Storage</span>:
          SWE-bench creates a Docker image <i>per task instance</i>.
          Across SWE-bench, SWE-gym, and R2E-gym, an image is
          on average 1GB - 3GB each.
          So with just 1000 tasks, you're talking at least 1TB of storage.
          That's a lot.
          To use the dataset, it's extremely time consuming to re-build the images,
          and not everyone has TBs of readily available storage.
        </li>
      </ol>
      <h3 class="fire-text">The Key Idea</h3>
      <p>
        After releasing SWE-bench Multimodal last September, for several weeks,
        I tried a bunch of ideas around automating more of SWE-bench's collection process.
      </p>
      <p>
        While it's not impossible to combine good heuristics with LMs to automate some parts,
        I became increasingly convinced that the fundamental approach of SWE-bench could
        be reworked.
      </p>
      <p>
        One thought that stood out to me was that while dependencies and
        installation procedures change over time, the codebase itself is typically "linear".
        Code that was written in 2021 is usually still there in 2025.
        PRs are merged at different times; their effect, however, remains present
        in the latest version of the codebase.
      </p>
      <p>
        So, to create a SWE-bench style task instance,
        instead of reverting the codebase to the PR's commit,
        why not undo the PR in the latest version of the code?
      </p>
      <p>
        This was the start of <span class="fire-text">SWE-smith</span>, a
        rethinking of how to create software tasks for training AI.
      </p>
      <blockquote>
        SWE-bench identifies task instances first, then attempts to build an
        environment for it.
        <br><br>
        Instead, define the execution environment <i>first</i>,
        then synthesize task instances within the environment.
      </blockquote>
      <p>
        So what does this mean in practice?
      </p>
      <p>
        Given a repository, let's start by determine the installation +
        testing specifications and create the Docker image.
      </p>
      <p>
        Then, given <i>just</i> this commit/version of the codebase,
        use automatic methods to change code in ways that are likely to break existing tests.
      </p>
      <p>
        The implications of this are meaningful. This means...
      </p>
      <ol>
        <li><span style="color:#00FF84;">Significantly reduced human labor</span>:
          No more figuring out to install a repo correctly for every month
          of the past 5 years.
        </li>
        <li><span style="color:#00FF84;">Significantly reduced storage</span>:
          One docker image per repository, not per task instance.
          For 1000 tasks, 1GB of storage, not 1TB.
        </li>
        <li><span style="color:#00FF84;">New ways to create bugs</span>:
          To break existing tests, we can revert PRs.
          Or you could ask an LM to write bugs into the code!
          Or maybe, remove a function entirely.
          Or if you have two bugs, merge them into a single, harder bug?
          The possibilities are suddenly endless.
        </li>
      </ol>
      <div class="carousel">
        <div class="slides">
          <div class="slide">
            <img src="assets/lm_generate.png">
            <div class="caption">Prompt an LM to write bugs into the code</div>
          </div>
          <div class="slide">
            <img src="assets/procedural.png">
            <div class="caption">Convert Function â†’ AST, mess with the AST</div>
          </div>
          <div class="slide">
            <img src="assets/pr_mirror.png">
            <div class="caption">Use an LM to undo changes made by a PR</div>
          </div>
          <div class="slide">
            <img src="assets/combine.png">
            <div class="caption">Merge different bugs together</div>
          </div>
        </div>
        <div class="nav-buttons">
          <button onclick="prevSlide()">&#8592;</button>
          <button onclick="nextSlide()">&#8594;</button>
        </div>
      </div>
      <p>
        Concretely, the workflow is as follows:
      </p>
      <p>
        To create an execution environment for a repo, I prompt
        SWE-agent to install the codebase and run the tests.
      </p>
      <p>
        It then takes me 7 minutes to read SWE-agent's work, figure out the
        right steps, and build the Docker image for that repository.
      </p>
      <p>
        Then, using the bug generation methods shown above, I can effortlessly
        create 100s to 1000s of task instances per repo.
      </p>
      <h3 class="fire-text">Deliverables & Great Expectations</h3>
      <p>
        Using SWE-smith, we create 50k task instances across 128 Python repositories (and counting...).
        Both numbers are an order of magnitude greater than any existing dataset.
      </p>
      <p>
        We train <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct">Qwen 2.5 32B Coder Instruct</a>
        on 5000 expert trajectories generated from running SWE-agent + Claude 3.7 Sonnet on a subset of SWE-smith.
        Our model, <a href="https://huggingface.co/SWE-bench/SWE-agent-LM-32B">SWE-agent-LM-32B</a>, achieves a 40% single run resolve rate on SWE-bench Verified,
        #1 for open source agentic coding models.
      </p>
    </section>
  </main>

  <script>
    const slides = document.querySelector('.slides');
    const totalSlides = document.querySelectorAll('.slide').length;
    let index = 0;

    function showSlide(i) {
      index = (i + totalSlides) % totalSlides;
      slides.style.transform = `translateX(-${index * 100}%)`;
    }

    function nextSlide() {
      showSlide(index + 1);
    }

    function prevSlide() {
      showSlide(index - 1);
    }

    // Optional: autoplay
    // setInterval(nextSlide, 5000);
  </script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-46KMJE3755"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-46KMJE3755');
  </script>
</body>
</html>
